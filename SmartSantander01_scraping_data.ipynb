{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping data from [OrganiCity](http://organicity.eu/)- [SmartSantander](http://www.smartsantander.eu/)\n",
    "\n",
    "\n",
    "[Discovery API](https://discovery.organicity.eu/) can be used to find entities of different cities. Once, the entities are determined it is possible to get [historic data](http://data.organicity.eu/) through a more straightforward option.   \n",
    "\n",
    "### Basic queries\n",
    "\n",
    "| Description | Query   |\n",
    "|------|------|\n",
    "|Filter assets from a site by type | https://discovery.organicity.eu/v0/assets/sites/santander?type=< TYPE \\> |\n",
    "|Query attributes of an asset with historical values | https://data.organicity.eu/< ASSET_ID \\>/attributes |\n",
    "|Query historical values from attribute in an asset | https://data.organicity.eu/< ASSET_ID \\>/< ATTRIBUTE \\>/readings |\n",
    "\n",
    "### SmartSantander types\n",
    "\n",
    "| Type |Attributes with historical data|\n",
    "|------|------|\n",
    "|urn:oc:entityType:repeater:light | illuminance|\n",
    "|urn:oc:entityType:iotdevice:vehicleSpeed | speed:median|\n",
    "|urn:oc:entityType:iotdevice:magneticLoop | trafficIntensity, roadOccupancy, roadLoad|\n",
    "|urn:oc:entityType:bikeStop | freeBikes, feeSpaces|\n",
    "|urn:oc:entityType:iotdevice:irrigation | temperature:ambient, relativeHumidity, temperature:ground, tension:soilMoisture|\n",
    "|urn:oc:entityType:mobileSensor | position:altitude, speed:instantaneous, direction:heading, mileage:total, chemicalAgentAtmosphericConcentration:CO, chemicalAgentAtmosphericConcentration:airParticles, temperature:ambient, relativeHumidity, chemicalAgentAtmosphericConcentration:O3|\n",
    "|urn:oc:entityType:repeater:air | co_index, temperature:ambient|\n",
    "|urn:oc:entityType:iotdevice:environmentalStation | temperature:ambient, humidity:relative, solarRadiation, rainFall, wind:speed, wind:direction, atmosphericPressure|\n",
    "|urn:oc:entityType:repeater:temp | temperature:ambient|\n",
    "|urn:oc:entityType:iotdevice:agriculture | temperature:ambient, relativeHumidity|\n",
    "|urn:oc:entityType:repeater:noise | soundPressureLevel:ambient|\n",
    "|urn:oc:entityType:iotdevice:parking | presenceStatus:parking|\n",
    "\n",
    "### Examples\n",
    "\n",
    "| Description | Query |\n",
    "|------|------|\n",
    "|Query assets in Santander of type urn:oc:entityType:iotdevice:magneticLoop. Page size is 10 and we query the 10th page  | https://discovery.organicity.eu/v0/assets/sites/santander?type=urn:oc:entityType:iotdevice:parking&per=10&page=10 |\n",
    "|Query attributes with historical from asset urn:oc:entity:santander:traffic:magneticLoop:1004 | https://data.organicity.eu/urn:oc:entity:santander:parking:np3711/attributes |\n",
    "|Query recorded values of attribute roadOccupancy from asset urn:oc:entity:santander:parking:np3711 for the Januar 2018 | https://data.organicity.eu/urn:oc:entity:santander:parking:np3711/presenceStatus:parking/readings?from=2018-01-01&to=2018-01-31 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import ssl\n",
    "import bs4 as bs\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "#set maximum lines to display when printing dataframe\n",
    "pd.options.display.max_rows = 8\n",
    "#To save data\n",
    "import csv\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is  30  of urn:oc:entityType:repeater:light\n",
      "There is  21  of urn:oc:entityType:iotdevice:vehicleSpeed\n",
      "There is  30  of urn:oc:entityType:iotdevice:magneticLoop\n",
      "There is  16  of urn:oc:entityType:bikeStop\n",
      "There is  12  of urn:oc:entityType:iotdevice:irrigation\n",
      "There is  30  of urn:oc:entityType:mobileSensor\n",
      "There is  1  of urn:oc:entityType:repeater:air\n",
      "There is  3  of urn:oc:entityType:iotdevice:environmentalStation\n",
      "There is  22  of urn:oc:entityType:repeater:temp\n",
      "There is  9  of urn:oc:entityType:iotdevice:agriculture\n",
      "There is  30  of urn:oc:entityType:repeater:noise\n",
      "There is  30  of urn:oc:entityType:iotdevice:parking\n",
      "The total number of sensors is  234\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Getting the entire available data set. The first step is to get Id and location for each device\n",
    "\"\"\"\n",
    "\n",
    "list_of_sensor_types = [\"urn:oc:entityType:repeater:light\", \"urn:oc:entityType:iotdevice:vehicleSpeed\", \n",
    "                      \"urn:oc:entityType:iotdevice:magneticLoop\", \"urn:oc:entityType:bikeStop\", \n",
    "                      \"urn:oc:entityType:iotdevice:irrigation\", \"urn:oc:entityType:mobileSensor\", \n",
    "                      \"urn:oc:entityType:repeater:air\", \"urn:oc:entityType:iotdevice:environmentalStation\", \n",
    "                      \"urn:oc:entityType:repeater:temp\", \"urn:oc:entityType:iotdevice:agriculture\", \n",
    "                      \"urn:oc:entityType:repeater:noise\", \"urn:oc:entityType:iotdevice:parking\"]\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "dfsensors = pd.DataFrame()\n",
    "#The data we read from types\n",
    "readings_types = [\"id\",\"latitude\",\"longitude\"]\n",
    "\n",
    "#total number of sensors\n",
    "total = 0\n",
    "\n",
    "for sensor_type in list_of_sensor_types:\n",
    "    url = \"https://discovery.organicity.eu/v0/assets/sites/santander?type=\" + sensor_type\n",
    "    #print(url)\n",
    "    connection = urllib.request.urlopen(url, context=ctx)\n",
    "    sauce = urllib.request.urlopen(url, context=ctx)\n",
    "    soup = bs.BeautifulSoup(sauce , 'html.parser')\n",
    "    #print(soup)\n",
    "    data_string = soup.get_text()\n",
    "    #print(data_string)\n",
    "\n",
    "    #Data cleaning\n",
    "    #remove start and end of data_string\n",
    "    data_string = data_string.replace(\"[{\",\"\")\n",
    "    data_string = data_string.replace(\"]}\",\"\")\n",
    "    \n",
    "    #Counting sensors\n",
    "    sensor_data = data_string.split(\"},{\")\n",
    "    print(\"There is \", len(sensor_data), \" of\", sensor_type)\n",
    "    total = total + len(sensor_data)\n",
    "    #remove site information from data\n",
    "    sep = '\\\"site\\\"'\n",
    "    sensor_data = [x.split(sep, 1)[0] for x in sensor_data]\n",
    "\n",
    "    #Get, id, and position (latitude and longitude)\n",
    "    for sensor in sensor_data:\n",
    "        sensor = sensor.split(\"\\\"\")\n",
    "        sensor = list(filter(lambda a: a != ':', sensor))\n",
    "        sensor_values = [sensor[sensor.index(x)+1] for x in readings_types]\n",
    "        df_line = pd.DataFrame([sensor_values], columns=readings_types)\n",
    "        dfsensors = dfsensors.append(df_line, ignore_index=True)\n",
    "\n",
    "#Print the total number of sensors\n",
    "print(\"The total number of sensors is \", total)\n",
    "        \n",
    "#Change coordinates from string to float\n",
    "dfsensors.longitude = [float(x[1:len(x)-1]) for x in dfsensors.longitude]\n",
    "dfsensors.latitude = [float(x[1:len(x)-1]) for x in dfsensors.latitude]\n",
    "#Make sure that coordinates are ok, Note that check is limited to Western Europe\n",
    "dfsensors[\"correct_longitude\"] = [x if x < 0 else y for x,y in zip(dfsensors.longitude, dfsensors.latitude) ]\n",
    "dfsensors[\"correct_latitude\"] = [x if x > 0 else y for x,y in zip(dfsensors.longitude, dfsensors.latitude) ]\n",
    "dfsensors = dfsensors.drop(columns=['longitude', 'latitude'])\n",
    "\n",
    "dfsensors.columns = [\"id\",\"longitude\",\"latitude\"]\n",
    "\n",
    "#Add line for ID number, to have just a number as identifier\n",
    "dfsensors[\"id_number\"] = [(x.split(\":\")[-2][0]).upper() + re.findall(r'\\d+', x)[0] for x in dfsensors.id]\n",
    "\n",
    "#Information to connect to page with data\n",
    "user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'\n",
    "headers={'User-Agent':user_agent,} \n",
    "\n",
    "#Create dataframes for all sensor measurment available\n",
    "df_all_sensor_data = pd.DataFrame()\n",
    "\n",
    "for index, sensor_data in dfsensors.iterrows():\n",
    "    url = \"https://data.organicity.eu/\" + sensor_data.id + \"/attributes\"\n",
    "    #print(url)\n",
    "    request= urllib.request.Request(url,None,headers) #The assembled request\n",
    "    sauce = urllib.request.urlopen(request)\n",
    "    soup = bs.BeautifulSoup(sauce , 'html.parser')\n",
    "    data_string = soup.get_text()\n",
    "    #print(data_string)\n",
    "    number_of_attributes = 0\n",
    "    #Data cleaning\n",
    "    data_string = data_string.replace(\"{\\\"attributes\\\":[\",\"\")\n",
    "    data_string = data_string.replace(\"]}\",\"\")\n",
    "    #Get attributes\n",
    "    attributes  = data_string.split(\",\")\n",
    "    attributes = [x[1:-1] for x in attributes]\n",
    "    \n",
    "    #Create dataframes for actual sensor values, to data for every sensor seperately\n",
    "    df_one_sensor = pd.DataFrame()\n",
    "\n",
    "    if  attributes[0] == '':\n",
    "        #print(\"No attributes at node \" + str(sensor_data.id_number))\n",
    "        continue \n",
    "    else:\n",
    "        number_of_attributes = len(attributes)\n",
    "        #print( \"Attributes at node \" + str(sensor_data.id_number) + \": \" +(''.join(str(x) + \", \" for x in attributes))[:-2])\n",
    "        for attribute in attributes:\n",
    "            #querry is the url\n",
    "            url = \"https://data.organicity.eu/\" + sensor_data.id + \"/\" + attribute +\"/readings\"\n",
    "            request= urllib.request.Request(url,None,headers) #The assembled request\n",
    "            sauce = urllib.request.urlopen(request)\n",
    "            soup = bs.BeautifulSoup(sauce , 'html.parser')\n",
    "            data_string = soup.get_text()\n",
    "\n",
    "            #Data cleaning\n",
    "            #remove start and end of data because it is necesary\n",
    "            data_string = data_string.replace(\"{\\\"readings\\\":[{\",\"\")\n",
    "            data_string = data_string.replace(\"}]}\",\"\")\n",
    "            #remove extra quotation marks\n",
    "            data_string = data_string.replace(\"\\\"\", \"\")\n",
    "            data_string = data_string.replace(\"\", \"\")\n",
    "            data_string = data_string.replace(\"}]}\",\"\")\n",
    "            measurments = data_string.split(\"},{\")\n",
    "\n",
    "            for each_measurment in measurments:\n",
    "                s = each_measurment.split(\",\")\n",
    "                measur_readings_types = []\n",
    "                measur_values = []\n",
    "                for test in s:\n",
    "                    measur_readings_types.append(test.split(\":\",1)[0])\n",
    "                    measur_readings_types.append(\"attribute\")\n",
    "                    measur_values.append(test.split(\":\",1)[1])\n",
    "                    measur_values.append(attribute)\n",
    "                #print(s[1].split(\":\"))    \n",
    "                df_line = pd.DataFrame([measur_values], columns=measur_readings_types)\n",
    "                #print(df_line)\n",
    "                df_one_sensor = df_one_sensor.append(df_line, ignore_index=True)\n",
    "\n",
    "        #Turn recvTime into a more usefull time tag, depending on application\n",
    "        df_one_sensor.recvTime  = pd.to_datetime(df_one_sensor.recvTime, format=\"%Y-%m-%dT%H:%M:%S\")\n",
    "        date, time = zip(*[(d.date(), d.time()) for d in df_one_sensor['recvTime']])\n",
    "        df_one_sensor = df_one_sensor.assign(date = date, time =time)\n",
    "        df_one_sensor.index = pd.to_datetime(df_one_sensor['date'])\n",
    "        df_one_sensor = df_one_sensor.drop('recvTime', 1)\n",
    "        df_one_sensor = df_one_sensor.drop('date', 1)\n",
    "        #print(df_one_sensor)\n",
    "        #Save to a file\n",
    "        df_one_sensor.to_csv(\"./scraped_data/\" + sensor_data.id_number, sep='\\t')\n",
    "        #Save measurment to a dataframe saving all measurments\n",
    "        df_all_sensor_data = df_all_sensor_data.append(df_one_sensor, ignore_index=True)\n",
    "        \n",
    "    #data_string = \"illuminance\"\n",
    "    \n",
    "    dfsensors[\"num_attributes\"] = number_of_attributes\n",
    "    dfsensors[\"attributes\"] = (''.join(str(x) + \", \" for x in attributes))[:-2]\n",
    "#Save the two main files to a file\n",
    "df_all_sensor_data.to_csv(\"./scraped_data/all_sensor_data\", sep='\\t')\n",
    "dfsensors.to_csv(\"./scraped_data/all_sensor_metadata\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           attrValue             time\n",
      "date                                 \n",
      "2018-01-01     false  14:43:58.480000\n",
      "2018-01-01      true  14:58:59.019000\n",
      "2018-01-01     false  23:22:59.623000\n",
      "2018-01-02     false  11:08:01.175000\n",
      "...              ...              ...\n",
      "2018-01-22     false  09:41:53.118000\n",
      "2018-01-22     false  13:44:52.499000\n",
      "2018-01-24      true  10:12:56.475000\n",
      "2018-01-24     false  10:29:57.080000\n",
      "\n",
      "[106 rows x 2 columns]\n",
      "           attrValue             time\n",
      "date                                 \n",
      "2018-01-01     false  14:43:58.480000\n",
      "2018-01-01      true  14:58:59.019000\n",
      "2018-01-01     false  23:22:59.623000\n",
      "2018-01-02     false  11:08:01.175000\n",
      "...              ...              ...\n",
      "2018-01-02     false  14:53:12.123000\n",
      "2018-01-02      true  15:15:02.229000\n",
      "2018-01-02     false  16:02:01.226000\n",
      "2018-01-02      true  19:45:01.757000\n",
      "\n",
      "[13 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "A simple example to read data and save data to .csv of attribute roadOccupancy from asset \n",
    "urn:oc:entity:santander:parking:np3711 for the Januar 2018 \n",
    "\"\"\"\n",
    "user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'\n",
    "headers={'User-Agent':user_agent,} \n",
    "\n",
    "#querry is the url\n",
    "url = \"https://data.organicity.eu/urn:oc:entity:santander:parking:np3711/presenceStatus:parking/readings?from=2018-01-01&to=2018-01-31\"\n",
    "\n",
    "request= urllib.request.Request(url,None,headers) #The assembled request\n",
    "sauce = urllib.request.urlopen(request)\n",
    "\n",
    "soup = bs.BeautifulSoup(sauce , 'html.parser')\n",
    "\n",
    "#print(soup)\n",
    "\n",
    "data_string = soup.get_text()\n",
    "#print(data_string)\n",
    "\n",
    "#Data cleaning\n",
    "#remove start and end of data because it is necesary\n",
    "data_string = data_string.replace(\"{\\\"readings\\\":[{\",\"\")\n",
    "data_string = data_string.replace(\"}]}\",\"\")\n",
    "#remove extra quotation marks\n",
    "data_string = data_string.replace(\"\\\"\", \"\")\n",
    "data_string = data_string.replace(\"\", \"\")\n",
    "data_string = data_string.replace(\"}]}\",\"\")\n",
    "measurments = data_string.split(\"},{\")\n",
    "\n",
    "#print(measurments)\n",
    "\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for each_measurment in measurments:\n",
    "    s = each_measurment.split(\",\")\n",
    "    measur_readings_types = []\n",
    "    measur_values = []\n",
    "    for test in s:\n",
    "        measur_readings_types.append(test.split(\":\",1)[0])\n",
    "        measur_values.append(test.split(\":\",1)[1])\n",
    "    #print(s[1].split(\":\"))    \n",
    "    df_line = pd.DataFrame([measur_values], columns=measur_readings_types)\n",
    "    #print(df_line)\n",
    "    df = df.append(df_line, ignore_index=True)\n",
    "    \n",
    "#Turn recvTime into a more usefull time tag, depending on application\n",
    "df.recvTime  = pd.to_datetime(df.recvTime, format=\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "date, time = zip(*[(d.date(), d.time()) for d in df['recvTime']])\n",
    "df = df.assign(date = date, time =time)\n",
    "\n",
    "#year, month, day, hour, minutes, seconds = zip(*[(d.year, d.month, d.day, d.hour, d.minute, d.second) for d in df['recvTime']])\n",
    "#df = df.assign(year=year, month = month, day = day, hour=hour, minutes = minutes, seconds = seconds)\n",
    "\n",
    "df.index = pd.to_datetime(df['date'])\n",
    "df = df.drop('recvTime', 1)\n",
    "df = df.drop('date', 1)\n",
    "print(df)\n",
    "#exaple, how to filter measurments based on the date\n",
    "filtered = df['2018-01-01':'2018-01-02']\n",
    "print(filtered)\n",
    "#Save to a file\n",
    "filtered.to_csv(\"./scraped_data/filtered_data\", sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
